
Woosuk Kwon¹,∗ Zhuohan Li¹,∗ Siyuan Zhuang¹ Ying Sheng¹,² Lianmin Zheng¹ Cody Hao Yu³ Joseph E. Gonzalez¹ Hao Zhang⁴ Ion Stoica¹  
¹UC Berkeley ²Stanford University ³Independent Researcher ⁴UC San Diego

## 초록

대량의 언어 모델(LLMs)을 고처리량으로 서비스하기 위해서는 한 번에 충분히 많은 요청을 묶어야 합니다. 그러나 기존 시스템은 각 요청에 대한 키-값 캐시(KV 캐시) 메모리가 크고 동적으로 증가하고 감소하기 때문에 어려움을 겪습니다. 비효율적으로 관리되면, 이 메모리는 단편화와 중복 복제로 인해 크게 낭비되어 배치 크기를 제한합니다. 이 문제를 해결하기 위해, 우리는 운영 체제의 고전적인 가상 메모리와 페이징 기술에서 영감을 받은 주의 알고리즘인 PagedAttention을 제안합니다. 이를 바탕으로, 우리는 KV 캐시 메모리의 낭비를 거의 없애고 요청 내 및 요청 간에 KV 캐시를 유연하게 공유하여 메모리 사용을 더욱 줄이는 LLM 서비스 시스템인 vLLM을 구축합니다. 우리의 평가에 따르면 vLLM은 FasterTransformer 및 Orca와 같은 최신 시스템에 비해 동일한 수준의 지연 시간으로 인기 있는 LLM의 처리량을 2-4배 향상시킵니다. 이 개선은 더 긴 시퀀스, 더 큰 모델 및 더 복잡한 디코딩 알고리즘에서 더욱 두드러집니다. vLLM의 소스 코드는 [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)에서 공개적으로 이용할 수 있습니다.

## 1. 서론

GPT[5, 37] 및 PaLM[9]과 같은 대규모 언어 모델(LLMs)의 출현은 프로그래밍 보조[6, 18]와 범용 채팅 봇[19, 35]과 같은 새로운 응용 프로그램을 가능하게 했으며, 이는 우리의 작업과 일상에 깊은 영향을 미치기 시작했습니다. 많은 클라우드 기업들이 이러한 응용 프로그램을 호스팅 서비스로 제공하기 위해 경쟁하고 있습니다[34, 44]. 그러나 이러한 응용 프로그램을 실행하는 것은 매우 비용이 많이 들며, GPU와 같은 대량의 하드웨어 가속기가 필요합니다. 최근 추정에 따르면, LLM 요청을 처리하는 것은 전통적인 키워드 쿼리보다 10배 더 비용이 많이 들 수 있습니다[43]. 이러한 높은 비용을 감안할 때, LLM 서비스 시스템의 처리량을 증가시켜 요청당 비용을 줄이는 것이 더욱 중요해지고 있습니다.

LLMs의 핵심에는 Self-Attention 기반의 Autoregressive Transformer 모델[53]이 있습니다. 이 모델은 입력(prompt)과 지금까지 생성된 출력의 이전 시퀀스를 기반으로 단어(토큰)를 하나씩 생성합니다. 각 요청에 대해 이 비용이 많이 드는 프로세스가 종료 토큰을 출력할 때까지 반복됩니다. 이 순차적 생성 프로세스는 작업 부하를 메모리 중심으로 만들어 GPU의 계산 능력을 충분히 활용하지 못하고 서비스 처리량을 제한합니다.

처리량을 개선하는 것은 여러 요청을 함께 묶음으로써 가능합니다. 그러나 많은 요청을 배치로 처리하기 위해서는 각 요청의 메모리 공간을 효율적으로 관리해야 합니다. 예를 들어, 그림 1(왼쪽)은 NVIDIA A100 GPU에서 13B-parameter LLM의 메모리 분포를 보여줍니다. 약 65%의 메모리가 모델 가중치에 할당되며, 이는 서비스 중에 정적 상태로 유지됩니다. 약 30%의 메모리가 요청의 동적 상태를 저장하는 데 사용됩니다. Transformer에서는 이 상태가 주의 메커니즘과 관련된 키와 값 텐서로 구성되며, 일반적으로 KV 캐시[41]라고 불리며, 이는 시퀀스 내에서 새로운 출력 토큰을 생성하기 위한 이전 토큰의 컨텍스트를 나타냅니다. 나머지 소량의 메모리는 일시적으로 활성화에 사용됩니다.

그림 1: NVIDIA A100에서 13B 파라미터를 가진 LLM을 서비스할 때의 메모리 레이아웃. 파라미터(회색)는 서비스 동안 GPU 메모리에 지속적으로 유지됩니다. KV 캐시(빨간색)의 메모리는 서비스 요청 시 할당 및 해제됩니다. 소량의 메모리(노란색)는 활성화에 일시적으로 사용됩니다. 오른쪽: vLLM은 기존 시스템에서 볼 수 있는 KV 캐시 메모리의 급격한 성장 곡선을 완화하여 서비스 처리량을 현저히 증가시킵니다.


## 1. 서론

본 논문에서는 대규모 언어 모델(LLM)의 효율적인 메모리 관리를 위한 새로운 접근 방식을 제안합니다. 기존의 LLM 서비스 시스템은 KV 캐시 메모리를 비효율적으로 관리하여 내부 및 외부 메모리 단편화 문제를 야기합니다. 이는 요청의 최대 길이에 맞춰 연속적인 메모리 공간을 미리 할당하기 때문입니다. 이러한 문제를 해결하기 위해, 우리는 운영 체제(OS)의 가상 메모리와 페이징에서 영감을 받은 PagedAttention 알고리즘을 제안합니다. PagedAttention은 요청의 KV 캐시를 고정된 수의 토큰을 포함할 수 있는 블록으로 나누고, 이들 블록을 반드시 연속된 공간에 저장하지 않습니다.

그림 2: 실험 §6.2에서 다양한 LLM 서비스 시스템에서 평균 메모리 낭비 비율.

PagedAttention은 OS의 가상 메모리처럼 KV 캐시를 유연하게 관리할 수 있게 해주며, 블록을 페이지로, 토큰을 바이트로, 요청을 프로세스로 생각할 수 있습니다. 이 디자인은 비교적 작은 블록을 사용하고 필요에 따라 할당하여 내부 단편화를 완화합니다. 또한, 모든 블록의 크기가 같아 외부 단편화를 제거합니다. 마지막으로, 동일한 요청과 관련된 서로 다른 시퀀스나 심지어 서로 다른 요청 간에 블록 단위의 메모리 공유를 가능하게 합니다.

본 연구에서는 PagedAttention을 기반으로 높은 처리량을 자랑하는 분산 LLM 서비스 엔진인 vLLM을 구축하여 KV 캐시 메모리에서 거의 제로 낭비를 달성합니다. vLLM은 블록 수준의 메모리 관리와 PagedAttention과 공동 설계된 선점형 요청 스케줄링을 사용합니다. vLLM은 GPT[5], OPT[62], LLaMA[52]와 같은 인기 있는 LLM을 지원하며, 단일 GPU의 메모리 용량을 초과하는 모델도 포함합니다. 다양한 모델과 워크로드에 대한 평가 결과, vLLM은 기존 최첨단 시스템[31,60] 대비 LLM 서비스 처리량을 2-4배 향상시키며, 모델 정확도에는 전혀 영향을 미치지 않습니다. 개선 효과는 긴 시퀀스, 큰 모델 및 복잡한 디코딩 알고리즘에서 더욱 두드러집니다(§4.3).

요약하자면, 우리는 다음과 같은 기여를 합니다:
- LLM 서비스에서 메모리 할당의 문제점을 식별하고 서비스 성능에 미치는 영향을 정량화합니다.
- OS의 가상 메모리와 페이징에서 영감을 받아 비연속적인 페이지 메모리에 저장된 KV 캐시에서 작동하는 PagedAttention을 제안합니다.
- PagedAttention 위에 구축된 분산 LLM 서비스 엔진 vLLM을 설계하고 구현합니다.
- 다양한 시나리오에서 vLLM을 평가하고, FasterTransformer[31] 및 Orca[60]와 같은 기존 최첨단 솔루션보다 상당히 뛰어남을 입증합니다.

## 2. 배경

이 섹션에서는 일반적인 LLM의 생성 및 서비스 절차와 LLM 서비스에 사용되는 이터레이션 수준의 스케줄링을 설명합니다.

## 2.1 Transformer-Based Large Language Models

프롬프트 단계는 전체 사용자 프롬프트 $(x_1, \ldots, x_n)$를 입력으로 받아 첫 번째 새로운 토큰의 확률을 계산합니다. 언어 모델링의 과제는 다음 일련의 단어 $(x_1, \ldots, x_n)$의 확률을 모델링하는 것입니다. 이 과정은 조건부 확률의 곱으로 전체 시퀀스의 확률을 계산합니다 (일명 autoregressive decomposition [3]):

$$P(x) = P(x_1) \cdot P(x_2 | x_1) \cdots P(x_n | x_1, \ldots, x_{n-1}).$$

프롬프트 토큰 $x_1, \ldots, x_n$이 모두 알려져 있으므로, 프롬프트 단계의 계산은 행렬-행렬 곱셈 연산을 사용하여 병렬화할 수 있습니다. 따라서, 이 단계는 GPU에 내재된 병렬성을 효율적으로 사용할 수 있습니다.

Transformers [53]는 대규모로 위의 확률을 모델링하기 위한 사실상의 표준 아키텍처가 되었습니다. Transformer 기반 언어 모델의 가장 중요한 구성 요소는 Self-Attention 레이어입니다. 입력 숨겨진 상태 시퀀스 $(x_1, \ldots, x_n) \in \mathbb{R}^{n \times d}$에 대해, Self-Attention 레이어는 먼저 각 위치 $i$에 대해 쿼리, 키, 그리고 값 벡터를 얻기 위해 선형 변환을 적용합니다:

$$q_i = W_q x_i, \quad k_i = W_k x_i, \quad v_i = W_v x_i.$$

그 다음, Self-Attention 레이어는 한 위치의 쿼리 벡터와 그 이전의 모든 키 벡터를 곱하여 Attention 점수 $a_{ij}$를 계산하고, 값 벡터에 대한 가중 평균으로 출력 $o_i$를 계산합니다:

$$a_{ij} = \frac{\exp(q_i^\top k_j / \sqrt{d})}{\sum_{t=1}^{i} \exp(q_i^\top k_t / \sqrt{d})}, \quad o_i = \sum_{j=1}^{i} a_{ij} v_j.$$

## 2.2 LLM Service & Autoregressive Generation

훈련이 완료되면, LLM은 조건부 생성 서비스(e.g., completion API [34] 또는 chatbot [19,35])로 배포됩니다. LLM 서비스에 대한 요청은 입력 프롬프트 토큰 리스트 $(x_1, \ldots, x_n)$을 제공하며, LLM 서비스는 식 (1)에 따라 출력 토큰 리스트 $(x_{n+1}, \ldots, x_{n+T})$를 생성합니다. 우리는 프롬프트와 출력 리스트의 연결을 시퀀스라고 부릅니다. 식 (1)의 분해로 인해, LLM은 새 토큰을 하나씩 샘플링하고 생성할 수 있으며, 각 새 토큰의 생성 과정은 그 시퀀스의 모든 이전 토큰, 특히 그들의 키와 값 벡터에 의존합니다. 이 순차적 생성 과정에서 기존 토큰의 키와 값 벡터는 종종 미래 토큰 생성을 위해 캐시되며, 이를 KV 캐시라고 합니다. 한 토큰의 KV 캐시는 모든 이전 토큰에 의존합니다. 이는 시퀀스의 다른 위치에 나타나는 동일한 토큰의 KV 캐시가 다를 것임을 의미합니다.

요청 프롬프트가 주어지면, LLM 서비스의 생성 계산은 두 단계로 분해될 수 있습니다:

1. 프롬프트 단계: 전체 사용자 프롬프트를 입력으로 받아, 행렬-행렬 곱셈 연산을 사용하여 병렬화할 수 있는 확률을 계산합니다.
2. Autoregressive Generation 단계: 새로운 토큰을 순차적으로 생성합니다. $t$ 번째 반복에서, 모델은 하나의 토큰 $x_{n+t}$를 입력으로 받아, 이전 반복에서 캐시된 키 벡터 $k_1, \ldots, k_{n+t}$와 값 벡터 $v_1, \ldots, v_{n+t}$를 사용하여 다음 토큰의 확률 $P(x_{n+t+1} | x_1, \ldots, x_{n+t})$을 계산합니다. 이 단계는 시퀀스가 최대 길이에 도달하거나, <eos> 토큰이 생성될 때 완료됩니다. 서로 다른 반복에서의 계산은 데이터 의존성 때문에 병렬화될 수 없으며, 종종 덜 효율적인 행렬-벡터 곱셈을 사용합니다. 이로 인해, 이 단계는 GPU 계산을 심각하게 활용하지 못하고 메모리 제약을 받으며, 단일 요청의 지연 시간의 대부분을 차지합니다.

## 2.3 Batching Techniques for LLMs

LLM 서비스를 제공할 때의 계산 활용도를 개선하기 위해 여러 요청을 배치 처리할 수 있습니다. 요청이 동일한 모델 가중치를 공유하기 때문에, 가중치 이동에 대한 오버헤드는 배치 내의 요청에 걸쳐 균등하게 분산되며, 배치 크기가 충분히 큰 경우 계산 오버헤드에 의해 압도될 수 있습니다. 그러나, LLM 서비스에 요청을 배치하는 것은 두 가지 이유로 간단하지 않습니다. 첫째, 요청이 다른 시간에 도착할 수 있습니다. 단순한 배치 전략은 이전 요청이 나중 요청을 기다리게 하거나, 이전 요청이 완료될 때까지 들어오는 요청을 지연시켜 상당한 대기 지연을 초래할 수 있습니다. 둘째, 요청의 입력 및 출력 길이가 크게 다를 수 있습니다. 직관적인 배치 기술은 요청의 입력과 출력을 동일한 길이로 패딩하여 GPU 계산과 메모리를 낭비할 수 있습니다.

이 문제를 해결하기 위해, 세포형 배칭 [16]과 반복 수준 스케줄링 [60]과 같은 세밀한 배칭 메커니즘이 제안되었습니다. 전통적인 방법이 요청 수준에서 작동하는 것과는 달리, 이러한 기술은 반복 수준에서 작동합니다. 각 반복 후에, 완료된 요청은 배치에서 제거되고 새로운 요청이 추가됩니다. 따라서, 새로운 요청은 전체 배치가 완료될 때까지 기다리지 않고 단일 반복 후에 처리될 수 있습니다. 더욱이, 특별한 GPU 커널을 사용하면 입력과 출력을 패딩할 필요가 없습니다. 대기 지연과 패딩으로 인한 비효율성을 줄임으로써, 세밀한 배칭 메커니즘은 LLM 서비스의 처리량을 크게 증가시킵니다.

### PagedAttention 알고리즘

PagedAttention 알고리즘은 운영 체제의 페이징 개념에서 영감을 받은 Attention 알고리즘으로, §3에서 언급된 메모리 문제를 해결하기 위해 소개되었습니다. 전통적인 Attention 알고리즘과 달리, PagedAttention은 연속적인 키와 값 벡터를 비연속적인 메모리 공간에 저장할 수 있도록 합니다. 구체적으로, PagedAttention은 각 시퀀스의 KV 캐시를 KV 블록으로 분할합니다. 각 블록은 일정 수의 토큰에 대한 키와 값 벡터를 포함하며, 이를 KV 블록이라고 부릅니다.

#### 수식 4:

Attention 계산은 다음과 같이 블록 단위로 변형될 수 있습니다:

$$A_{ij} = \sum_{t=\lceil\frac{i}{B}\rceil}^{\lceil\frac{j}{B}\rceil} \text{exp}\left(\frac{q_i^\top K_t}{\sqrt{d}}\right)$$

$$ o_i = \sum_{j/\lceil B\rceil=1}^{j/\lceil B\rceil} V_j A_{ij}^\top $$

여기서 $A_{ij} = (a_{i,(j-1)B+1}, \ldots, a_{i,jB})$는 $j$번째 KV 블록에 대한 Attention 스코어의 행 벡터입니다.

PagedAttention 알고리즘은 KV 블록이 비연속적인 물리적 메모리에 저장될 수 있도록 하여, vLLM에서 더욱 유연한 페이징 메모리 관리를 가능하게 합니다.

### KVCacheManager

vLLM의 메모리 관리의 핵심 아이디어는 운영 체제의 가상 메모리 [25]와 유사합니다. 운영 체제는 메모리를 고정 크기의 페이지로 나누고 사용자 프로그램의 논리적 페이지를 물리적 페이지에 매핑합니다. 연속적인 논리적 페이지는 비연속적인 물리적 메모리 페이지에 대응될 수 있어, 사용자 프로그램이 메모리를 연속적으로 접근하는 것처럼 사용할 수 있게 합니다. 또한 물리적 메모리 공간을 미리 예약할 필요가 없으므로, 운영 체제는 필요에 따라 물리적 페이지를 동적으로 할당할 수 있습니다. vLLM은 LLM 서비스에서 KV 캐시를 관리하기 위해 가상 메모리의 아이디어를 사용합니다. PagedAttention을 통해, 우리는 KV 캐시를 가상 메모리의 페이지처럼 고정 크기의 KV 블록으로 구성합니다. 요청의 KV 캐시는 새로운 토큰과 그들의 KV 캐시가 생성될 때 왼쪽에서 오른쪽으로 채워지는 일련의 논리적 KV 블록으로 표현됩니다. 마지막 KV 블록의 채워지지 않은 위치는 미래의 생성들을 위해 예약됩니다.


## 물리적 KV 블록

### 그림 6: vLLM에서의 블록 테이블 변환

vLLM은 각 요청에 대해 물리적 KV 블록과 논리적 KV 블록 간의 매핑을 유지하는 블록 테이블을 관리합니다. 각 블록 테이블 항목은 논리적 블록의 해당 물리적 블록과 채워진 위치의 수를 기록합니다. 논리적 KV 블록과 물리적 KV 블록을 분리함으로써 vLLM은 모든 위치에 대해 메모리를 예약하지 않고도 KV 캐시 메모리를 동적으로 확장할 수 있습니다. 이는 기존 시스템에서 대부분의 메모리 낭비를 제거합니다.

### 그림 7: vLLM에서 동시에 두 요청의 KV 캐시 저장

PagedAttention과 vLLM을 사용한 디코딩 과정에서의 메모리 관리 방법을 설명하는 예제입니다. vLLM은 물리적 블록을 동적으로 할당하여 요청 내의 모든 메모리 낭비를 제한하고, 이를 통해 메모리를 효과적으로 활용할 수 있습니다. 이렇게 하면 더 많은 요청을 메모리에 배치할 수 있어 처리량이 향상됩니다. 한 요청이 생성이 완료되면, 해당 KV 블록은 다른 요청의 KV 캐시를 저장하기 위해 해제될 수 있습니다.

## 4.3 PagedAttention과 vLLM을 사용한 디코딩

PagedAttention 알고리즘과 vLLM을 사용하여 단일 입력 시퀀스의 디코딩 프로세스를 예로 설명합니다. vLLM은 OS의 가상 메모리처럼 최대 생성 시퀀스 길이에 대한 메모리를 미리 예약할 필요가 없습니다. 대신, 프롬프트 계산 중 생성된 KV 캐시를 수용하기 위해 필요한 KV 블록만 예약합니다. 예를 들어, 프롬프트에 7개의 토큰이 있을 경우, vLLM은 첫 번째 두 개의 논리적 KV 블록(0과 1)을 두 개의 물리적 KV 블록(7과 1)에 매핑합니다.

디코딩의 첫 번째 오토리그레시브 단계에서, PagedAttention 알고리즘으로 새 토큰을 생성합니다. 마지막 논리적 블록에 하나의 슬롯이 남아 있으므로, 새로 생성된 KV 캐시는 그곳에 저장되고, 블록 테이블의 채워진 기록이 업데이트됩니다. 두 번째 디코딩 단계에서, 마지막 논리적 블록이 가득 차면, vLLM은 새로 생성된 KV 캐시를 새로운 논리적 블록에 저장합니다. 이때 vLLM은 새로운 물리적 블록(물리적 블록 3)을 할당하고 이 매핑을 블록 테이블에 저장합니다.

## 4.4 다른 디코딩 시나리오에의 적용

PagedAttention과 vLLM이 단순한 디코딩 알고리즘(예: 탐욕적 디코딩 및 샘플링)을 어떻게 처리하는지 설명합니다. LLM 기반 프로그램 어시스턴트에서는 단일 입력 프롬프트에 대해 여러 샘플링된 출력을 생성하고, 사용자가 다양한 후보 중에서 선호하는 출력을 선택할 수 있습니다. vLLM은 메모리 공유 기회를 극대화하면서 복잡한 접근 패턴을 보이는 복잡한 디코딩 시나리오를 제공할 수 있습니다.

### Physical KV blocks

### Logical KV blocks

그림 8. 병렬 샘플링 예제. 샘플 공간. 알고리즘은 매 단계마다 보관할 최상위 후보의 수를 결정하는 빔 너비 매개변수 $k$에 의존합니다. 디코딩 중에 빔 검색은 빔 내의 각 후보 시퀀스를 확장하여 가능한 모든 토큰을 고려하고, LLM을 사용하여 각 토큰의 확률을 계산한 후, $k \cdot |V|$ 후보 중에서 가장 확률이 높은 상위 $k$ 시퀀스를 유지합니다. 여기서 $|V|$는 어휘의 크기입니다.

병렬 디코딩과 달리, 빔 검색은 초기 프롬프트 블록뿐만 아니라 다른 후보 간의 블록도 공유할 수 있으며, 디코딩 과정이 진행됨에 따라 공유 패턴이 동적으로 변화합니다. 이는 OS에서 복합 포크에 의해 생성된 프로세스 트리와 유사합니다. 그림 9는 $k = 4$인 빔 검색 예제에서 vLLM이 KV 블록을 관리하는 방법을 보여줍니다. 점선으로 표시된 반복 이전에 각 후보 시퀀스는 4개의 전체 논리 블록을 사용했습니다. 모든 빔 후보는 첫 번째 블록 0(즉, 프롬프트)을 공유합니다. 후보 3은 두 번째 블록부터 다른 후보들과 경로가 달라집니다. 후보 0-2는 처음 3개의 블록을 공유하고 네 번째 블록에서 갈라집니다. 다음 반복에서는 가장 확률이 높은 상위 4개의 후보가 모두 후보 1과 2에서 시작됩니다. 원래의 후보 0과 3은 더 이상 상위 후보에 포함되지 않으므로, 이들의 논리 블록은 해제되고, 해당 물리 블록의 참조 카운트가 감소합니다. vLLM은 참조 카운트가 0이 된 모든 물리 블록(블록 2, 4, 5, 8)을 해제합니다. 그런 다음 vLLM은 새로운 후보의 새로운 KV 캐시를 저장하기 위해 새 물리 블록(블록 9-12)을 할당합니다. 이제 모든 후보는 블록 0, 1, 3을 공유하고, 후보 0과 1은 블록 6을, 후보 2와 3은 블록 7을 추가로 공유합니다.

기존의 LLM 서비스 시스템은 빔 후보 간의 KV 캐시에 대한 빈번한 메모리 복사를 요구합니다. 예를 들어, 그림 9에서 점선 이후, 후보 3은 후보 2의 KV 캐시의 많은 부분을 복사해야 세대 작업을 계속할 수 있습니다. 이러한 빈번한 메모리 복사 오버헤드는 vLLM의 물리 블록 공유로 인해 크게 줄어듭니다. vLLM에서는 서로 다른 빔 후보의 대부분의 블록을 공유할 수 있습니다. 복사-쓰기 메커니즘은 새로 생성된 토큰이 이전에 공유된 블록 내에 있을 때만 적용됩니다. 이는 병렬 디코딩에서처럼 하나의 데이터 블록만 복사하는 것을 포함합니다.

공유 프리픽스. 일반적으로, LLM 사용자는 시스템 프롬프트 [36]로도 알려진 작업의 설명, 예시 입력 및 출력 등을 포함하는 (긴) 설명을 제공합니다. 설명은 실제 작업 입력과 연결되어 요청의 프롬프트를 형성합니다. LLM은 이 프롬프트를 기반으로 출력을 생성합니다.



**표 1: 모델 크기와 서버 구성.**


*모델 병렬 실행을 하더라도, 각 모델 샤드는 여전히 동일한 입력 토큰 세트를 처리하므로 동일한 위치에 대한 KVCache가 필요합니다. 따라서, vLLM은 중앙 스케줄러 내에서 단일 KV 캐시 관리자를 특징으로 합니다. 서로 다른 GPU 워커는 관리자뿐만 아니라 논리적 블록에서 물리적 블록으로의 매핑을 공유합니다. 이 공통 매핑을 통해 GPU 워커는 스케줄러가 각 입력 요청에 대해 제공하는 물리적 블록을 사용하여 모델을 실행할 수 있습니다. 각 GPU 워커는 동일한 물리적 블록 ID를 가지지만, 워커는 자신의 주의 헤드에 해당하는 KV 캐시의 일부만 저장합니다.*

각 단계에서 스케줄러는 먼저 배치 내 각 요청에 대한 입력 토큰 ID와 블록 테이블을 준비합니다. 다음으로, 스케줄러는 이 제어 메시지를 GPU 워커에게 방송합니다. 그런 다음 GPU 워커는 입력 토큰 ID를 사용하여 모델 실행을 시작합니다. 주의 레이어에서, GPU 워커는 제어 메시지의 블록 테이블에 따라 KV 캐시를 읽습니다. 실행 중에, GPU 워커는 스케줄러의 조정 없이 all-reduce 통신 프리미티브를 사용하여 중간 결과를 동기화합니다. 마지막으로, GPU 워커는 이번 반복의 샘플된 토큰을 스케줄러에게 다시 보냅니다. 요약하자면, GPU 워커는 메모리 관리에 대해 동기화할 필요가 없으며, 각 디코딩 반복의 시작 부분에서 스텝 입력과 함께 모든 메모리 관리 정보를 받기만 하면 됩니다.

**5 구현**

vLLM은 FastAPI[15] 프론트엔드와 GPU 기반 추론 엔진을 갖춘 종단 간 서비스 시스템입니다. 프론트엔드는 OpenAI API[34] 인터페이스를 확장하여 사용자가 각 요청에 대해 최대 시퀀스 길이, 빔 폭 $k$와 같은 샘플링 매개변수를 사용자 정의할 수 있도록 합니다. vLLM 엔진은 8.5K 라인의 Python 코드와 2K 라인의 C++/CUDA 코드로 작성되었습니다. 스케줄러와 블록 매니저를 포함한 제어 관련 구성 요소는 Python으로 개발하였고, PagedAttention과 같은 주요 작업을 위한 사용자 정의 CUDA 커널을 개발했습니다. 모델 실행기에서는 GPT[5], OPT[62], LLaMA[52]와 같은 인기 있는 LLM을 PyTorch[39]와 Transformers[58]를 사용하여 구현하였습니다. 분산된 GPU 워커 간의 텐서 통신을 위해 NCCL[32]을 사용합니다.

**5.1 커널 수준 최적화**

PagedAttention은 기존 시스템에서 효율적으로 지원되지 않는 메모리 액세스 패턴을 도입하므로, 이를 최적화하기 위해 여러 GPU 커널을 개발하였습니다. (1) 재구성과 블록 쓰기의 통합. 각 Transformer 레이어에서 새로운 KV 캐시는 블록으로 분할되어 블록 읽기에 최적화된 메모리 레이아웃으로 재구성된 후, 블록 테이블에 지정된 위치에 저장됩니다. 커널 실행 오버헤드를 최소화하기 위해 이를 단일 커널로 통합합니다. (2) 블록 읽기와 주의의 통합. FasterTransformer[31]의 주의 커널을 수정하여 블록 테이블에 따라 KV 캐시를 읽고 주의 작업을 즉시 수행합니다. 연속적인 메모리 액세스를 보장하기 위해 각 블록 읽기에 GPU 워프를 할당합니다. 또한 요청 배치 내 다양한 시퀀스 길이를 지원합니다. (3) 블록 복사의 통합. copy-on-write 메커니즘에서 발행된 블록 복사 작업은 불연속적인 블록에서 수행될 수 있습니다. cudaMemcpyAsync API를 사용하면 작은 데이터 이동이 여러 번 호출될 수 있습니다. 이를 완화하기 위해 서로 다른 블록의 복사 작업을 하나의 커널 실행으로 배치하는 커널을 구현합니다.

**5.2 다양한 디코딩 알고리즘 지원**

vLLM은 포크, 추가, 해제라는 세 가지 핵심 메서드를 사용하여 다양한 디코딩 알고리즘을 구현합니다. 포크 메서드는 기존 시퀀스에서 새 시퀀스를 생성합니다. 추가 메서드는 시퀀스에 새 토큰을 추가합니다. 마지막으로, 해제 메서드는 시퀀스를 삭제합니다. 예를 들어, 병렬 샘플링에서 vLLM은 포크 메서드를 사용하여 단일 입력 시퀀스에서 여러 출력 시퀀스를 생성합니다. 그런 다음 추가를 통해 각 반복에서 이러한 시퀀스에 새 토큰을 추가하고, 해제를 사용하여 종료 조건을 충족하는 시퀀스를 삭제합니다. vLLM은 빔 탐색 및 프리픽스 공유에서도 동일한 전략을 적용합니다. 우리는 미래의 디코딩 알고리즘도 이러한 메서드의 조합을 통해 지원할 수 있다고 믿습니다.

**6 평가**


## 6.2 기본 샘플링

vLLM의 성능을 기본 샘플링(요청당 하나의 샘플)으로 세 가지 모델과 두 가지 데이터셋에서 평가합니다. 그림 12의 첫 번째 행은 ShareGPT 데이터셋에 대한 결과를 보여줍니다. 이 곡선은 요청 속도가 증가함에 따라 지연 시간이 처음에는 점진적으로 증가하지만, 그 후 갑자기 폭발적으로 증가하는 것을 보여줍니다. 이는 요청 속도가 서비스 시스템의 용량을 초과할 때 대기열의 길이가 무한히 증가하고, 따라서 요청의 지연 시간도 증가하기 때문입니다.

ShareGPT 데이터셋에서 vLLM은 Orca(Oracle)과 비교하여 1.7배에서 2.7배 높은 요청 속도를 유지할 수 있으며, Orca(Max)와 비교하여 2.7배에서 8배 높은 요청 속도를 유지하면서 유사한 지연 시간을 유지합니다. 이는 vLLM의 PagedAttention이 메모리 사용을 효율적으로 관리하여 Orca보다 더 많은 요청을 배치할 수 있게 해주기 때문입니다. 예를 들어, 그림 13a에서 보듯이 OPT-13B의 경우 vLLM은 Orca(Oracle)보다 2.2배, Orca(Max)보다 4.3배 더 많은 요청을 동시에 처리합니다. FasterTransformer와 비교하여 vLLM은 최대 22배 더 높은 요청 속도를 유지할 수 있는데, FasterTransformer는 Orca(Max)처럼 세밀한 스케줄링 메커니즘을 활용하지 않고 메모리를 비효율적으로 관리하기 때문입니다.

그림 12의 두 번째 행과 그림 13b는 유사한 경향을 따르는 Alpaca 데이터셋에 대한 결과를 보여줍니다. 예외적으로 그림 12(f)에서는 vLLM이 Orca(Oracle) 및 Orca(Pow2)에 비해 덜 두드러진 이점을 보입니다. 이는 OPT-175B(표 1)의 모델 및 서버 구성으로 인해 KV 캐시를 저장할 수 있는 GPU 메모리 공간이 충분히 제공되며, Alpaca 데이터셋에는 짧은 시퀀스들이 있기 때문입니다. 이 설정에서는 Orca(Oracle)과 Orca(Pow2)도 메모리 관리의 비효율성에도 불구하고 대량의 요청을 일괄 처리할 수 있습니다. 결과적으로 시스템의 성능은 메모리 제약보다는 컴퓨팅 제약이 됩니다.

## 6.3 병렬 샘플링과 빔 서치

PagedAttention에서 메모리 공유의 효과를 병렬 샘플링과 빔 서치라는 두 가지 인기 있는 샘플링 방법으로 평가합니다. 병렬 샘플링에서는 요청 내의 모든 병렬 시퀀스가 프롬프트의 KV 캐시를 공유할 수 있습니다. 그림 14의 첫 번째 행에서 보듯이 샘플링할 시퀀스 수가 많아질수록 vLLM은 Orca 기준선에 비해 더 많은 개선을 가져옵니다. 마찬가지로 그림 14의 두 번째 행은 다양한 빔 너비로 빔 서치에 대한 결과를 보여줍니다. 빔 서치는 더 많은 공유를 허용하므로 vLLM은 더욱 뛰어난 성능 혜택을 보여줍니다. OPT-13B와 Alpaca 데이터셋에서 Orca(Oracle) 대비 vLLM의 개선은 기본 샘플링에서 1.3배에서 빔 너비 6인 빔 서치에서는 2.3배로 증가합니다.

그림 15는 공유를 통해 절약된 블록 수를 공유하지 않은 총 블록 수로 나눈 값으로 계산된 메모리 절약량을 보여줍니다. 병렬 샘플링에서는 6.1%에서 9.8%의 메모리 절약을, 빔 서치에서는 37.6%에서 55.2%의 메모리 절약을 보여줍니다. ShareGPT 데이터셋을 사용한 동일한 실험에서는 병렬 샘플링에서 16.2%에서 30.5%, 빔 서치에서 44.3%에서 66.3%의 메모리 절약을 볼 수 있었습니다.

## 6.4 공유된 프리픽스

그림 16. 입력 프롬프트가 공통 접두사를 공유하는 번역 작업. 접두사는 (a) 1개의 예시로 80개의 토큰을 포함하거나 (b) 5개의 예시로 341개의 토큰을 포함합니다.

그림 17. 챗봇 작업에서의 성능.

모델로는 다국어 모델인 LLaMA-13B [52]를 사용합니다. 작업으로는 WMT16 [4] 영어-독일어 번역 데이터셋을 사용하여 몇 가지 프롬프트를 포함하는 두 가지 접두사를 합성합니다. 첫 번째 접두사는 단일 예시(즉, one-shot)를 포함하고, 다른 접두사는 5개의 예시(즉, few-shot)를 포함합니다. 그림 16(a)에 나타난 바와 같이, vLLM은 one-shot 접두사가 공유될 때 Orca(Oracle)보다 1.67배 높은 처리량을 달성합니다. 더욱이, 더 많은 예시가 공유될 때(그림 16(b)), vLLM은 Orca(Oracle)보다 3.58배 높은 처리량을 달성합니다.

6.5 챗봇
챗봇 [8,19,35]은 LLM의 가장 중요한 응용 중 하나입니다. 챗봇을 구현하기 위해, 모델이 채팅 기록과 마지막 사용자 쿼리를 프롬프트로 연결하여 응답을 생성하도록 합니다. 우리는 ShareGPT 데이터셋을 사용하여 채팅 기록과 사용자 쿼리를 합성합니다. OPT-13B 모델의 제한된 컨텍스트 길이로 인해, 프롬프트를 마지막 1024개의 토큰으로 자르고, 모델이 최대 1024개의 토큰을 생성하도록 합니다. 우리는 대화 라운드 간에 KV 캐시를 저장하지 않으며, 이는 대화 라운드 간에 다른 요청을 위한 공간을 차지할 것이기 때문입니다. 그림 17은 vLLM이 세 가지 Orca 기준선보다 2배 높은 요청률을 유지할 수 있음을 보여줍니다. ShareGPT 데이터셋에는 많은 긴 대화가 포함되어 있기 때문에, 대부분의 요청에 대한 입력 프롬프트는 1024개의 토큰을 가지고 있습니다. Buddy 할당 알고리즘 때문에, Orca 기준선은 출력 길이를 예측하는 방식에 관계없이 요청 출력을 위한 1024개의 토큰 공간을 예약합니다. 이로 인해 세 개의 Orca 기준선은 유사하게 작동합니다. 반면, vLLM은 PagedAttention이 메모리 조각화 및 예약 문제를 해결하기 때문에 긴 프롬프트를 효과적으로 처리할 수 있습니다.

그림 18. Ablation 실험.

7 Ablation 연구
이 섹션에서는 vLLM의 다양한 측면을 연구하고 ablation 실험을 통해 우리가 내린 설계 선택을 평가합니다.

7.1 Kernel microbenchmark
PagedAttention의 동적 블록 매핑은 저장된 KV 캐시를 포함하는 GPU 작업의 성능에 영향을 미칩니다. 즉, 블록 읽기/쓰기와 Attention이 포함됩니다. 기존 시스템과 비교하여, 우리의 GPU 커널(§5)은 블록 테이블 접근, 추가 분기 실행, 가변 시퀀스 길이 처리의 추가 오버헤드가 포함됩니다. 그림 18a에 나타난 바와 같이, 이는 고도로 최적화된 FasterTransformer 구현에 비해 20–26% 더 높은 Attention 커널 지연을 초래합니다. 우리는 이 오버헤드가 Attention 연산자에만 영향을 미치고 모델의 다른 연산자, 예를 들어 Linear에는 영향을 미치지 않기 때문에 작다고 믿습니다. 오버헤드에도 불구하고, PagedAttention은 vLLM이 전체 성능에서 FasterTransformer를 크게 능가하도록 합니다(§6).

7.2 블록 크기의 영향
블록 크기의 선택은 vLLM의 성능에 상당한 영향을 미칠 수 있습니다. 블록 크기가 너무 작으면, vLLM은 KV 캐시를 읽고 처리할 때 GPU의 병렬성을 완전히 활용하지 못할 수 있습니다. 블록 크기가 너무 크면, 내부 조각화가 증가하고 공유 확률이 감소합니다.

그림 18b에서는 ShareGPT와 Alpaca 트레이스를 사용하여 다양한 블록 크기에서 vLLM의 성능을 평가합니다. ShareGPT 트레이스에서는 16에서 128까지의 블록 크기가 최고의 성능을 이끌어냅니다. Alpaca 트레이스에서는 블록 크기 16과 32가 잘 작동하지만, 더 큰 블록 크기는 성능을 크게 저하시키며, 시퀀스가 블록 크기보다 짧아집니다. 실제로, 우리는 블록 크기 16이 GPU를 효율적으로 활용하고 대부분의 작업에서 중요한 내부 조각화를 피하기에 충분히 큰 것으로 나타납니다. 따라서, vLLM은 기본 블록 크기를 16으로 설정합니다.

이 논문은 PagedAttention이라는 새로운 Attention 알고리즘을 제안하며, 이는 Attention 키와 값을 비연속적인 페이지 메모리에 저장할 수 있도록 하고, PagedAttention을 통해 효율적인 메모리 관리가 가능한 높은 처리량의 LLM 서빙 시스템인 vLLM을 소개합니다. 운영체제에서 영감을 받아, 가상 메모리와 Copy-on-Write와 같은 기존 기술이 어떻게 KV 캐시를 효율적으로 관리하고 LLM 서빙에서 다양한 디코딩 알고리즘을 처리하는 데 적응될 수 있는지를 보여줍니다. 우리의 실험은 vLLM이 최첨단 시스템 대비 2-4배의 처리량 개선을 달성함을 보여줍니다.

**Acknowledgement**

Xiaoxuan Liu, Zhifeng Chen, Yanping Huang, 익명의 SOS 프리뷰어들, 그리고 우리의 셰퍼드인 Lidong Zhou에게 그들의 통찰력 있는 피드백에 대해 감사드립니다. 이 연구는 Andreessen Horowitz, Anyscale, Astronomer, Google, IBM, Intel, Lacework, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Samsung SDS, Uber, 및 VMware의 기부로 부분적으로 지원되었습니다.

