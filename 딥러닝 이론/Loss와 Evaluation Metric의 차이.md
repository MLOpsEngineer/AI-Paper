Loss와 Evaluation Metric은 모두 모델의 성능을 측정하는 데 사용되지만, **목적**과 **역할**에서 차이가 있습니다. 이 두 개념을 조금 더 상세히 살펴보겠습니다.

---

## Loss Function (손실 함수)

### 1. 정의 및 목적
- Loss Function은 **모델의 예측값과 실제 타깃 값 사이의 차이를 수치화**하여 나타내는 함수입니다.
- 학습 단계에서 손실 함수를 통해 모델이 얼마나 잘 예측하고 있는지, 또는 잘못 예측하고 있는지를 측정하고, 이 값이 최소화되도록 모델의 **가중치(weight)**를 조정합니다.

### 2. 역할
- 손실 함수는 **모델을 학습시키기 위한 최적화 과정**에서 사용됩니다.
- 모델이 예측값과 실제 값 사이의 차이를 줄이기 위해 손실 함수를 최소화하는 방향으로 가중치를 조정하는데, 이 과정에서 **Gradient Descent**와 같은 최적화 알고리즘을 사용하여 손실을 줄입니다.
- 손실 함수는 모델의 **오차를 수학적으로 계산하고 미분 가능해야** 합니다. 미분 가능하다는 것은 모델이 오차를 줄이는 방향으로 나아갈 수 있도록 미분(Gradient)을 계산할 수 있어야 한다는 의미입니다.

### 3. 종류 및 예시
- **Mean Squared Error (MSE)**: 회귀 문제에서 많이 사용되며, 예측값과 실제값의 차이를 제곱해 평균화한 값입니다. 큰 오차에 대해 더 큰 페널티를 부과합니다.
- **Cross Entropy Loss**: 분류 문제에서 주로 사용되며, 예측 확률 분포와 실제 타깃 분포 사이의 차이를 나타냅니다. 확률값을 기준으로 계산되어, 실제 타깃 값에 대한 예측 확률이 높을수록 손실이 줄어듭니다.
- **Binary Cross Entropy**: 이진 분류 문제에서 사용되며, 0 또는 1로 라벨링된 실제 값과 예측 확률 값의 오차를 측정합니다.
- **Huber Loss**: 평균 제곱 오차와 평균 절대 오차의 장점을 결합한 손실 함수로, 예측 값이 실제 값과 크게 차이날 때 더 안정적인 성능을 보장합니다.

### 4. 특징
- 손실 함수는 모델의 **학습 중간 결과**를 측정하는 지표로, 이 값이 줄어드는 방향으로 학습이 진행됩니다.
- 손실 함수는 **수학적 성격**을 지니며, 학습이 진행되는 동안 모델이 어떻게 최적화되고 있는지를 수치적으로 나타냅니다.
- Loss가 감소한다고 해서 반드시 최종 모델 성능이 좋다는 것을 의미하지는 않습니다. 모델이 훈련 데이터에 너무 맞춰지면 Loss는 줄어들지만, 새로운 데이터에 대한 성능(일반화 성능)은 떨어질 수 있기 때문입니다.

---

## Evaluation Metric (평가지표)

### 1. 정의 및 목적
- **평가지표**는 학습된 모델이 새로운 데이터에서 얼마나 잘 작동하는지 평가하기 위해 사용되는 지표입니다.
- Loss Function과 달리 Evaluation Metric은 **모델이 훈련 및 검증 데이터에서 예측을 얼마나 잘 수행했는지**를 더 직관적이고 해석 가능한 방식으로 나타냅니다.

### 2. 역할
- 평가 지표는 **모델이 학습을 마친 후** 성능을 평가하고, 실제 사용 시 모델의 품질을 판단하는 데 도움을 줍니다.
- 모델의 성능을 다양한 관점에서 측정할 수 있도록 다양한 지표가 존재합니다. 예를 들어, 분류 문제에서는 **정확도(Accuracy)**나 **정밀도(Precision)**, **재현율(Recall)**, **F1 Score** 등을 사용하여 모델 성능을 평가할 수 있습니다.
- 평가 지표는 모델이 목표로 하는 특정 요구사항에 맞추어 설정될 수 있습니다. 예를 들어, 암 진단 모델에서는 재현율이 중요한 지표가 될 수 있습니다(암 환자를 놓치지 않도록).

### 3. 종류 및 예시
- **Accuracy (정확도)**: 전체 예측 중에서 맞춘 비율로, 단순하고 직관적입니다. 하지만 불균형한 데이터셋에서는 부적절할 수 있습니다.
- **Precision (정밀도)**: 예측한 결과 중 실제로 맞은 비율을 의미합니다. False Positive를 줄이는 것이 중요할 때 사용됩니다.
- **Recall (재현율)**: 실제 Positive 중에서 모델이 정확히 예측한 비율로, False Negative를 줄이는 것이 중요할 때 사용됩니다.
- **F1 Score**: Precision과 Recall의 조화 평균으로, 두 지표 간의 균형이 중요할 때 유용합니다.
- **ROC-AUC Score**: 모델의 예측 성능을 곡선 아래 면적(AUC)으로 측정하며, 1에 가까울수록 이상적인 분류를 의미합니다.

### 4. 특징
- 평가 지표는 **모델 성능을 평가하고 비교하는 데 있어 더 직관적**입니다.
- 학습 단계에서 손실 함수를 최적화하는 동안 최종 모델의 성능을 평가하는 것은 주로 평가 지표를 통해 이루어집니다.
- Loss가 낮더라도 Evaluation Metric이 낮을 수 있는데, 이는 모델이 특정 상황에 과적합되어 실제 데이터에서 잘 작동하지 않을 수 있기 때문입니다.

---

## Loss Function과 Evaluation Metric의 주요 차이점

| 구분                 | Loss Function                                                                                   | Evaluation Metric                                                         |
|----------------------|-------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------|
| **역할**              | 모델 학습 단계에서 **가중치 조정**을 위해 오차를 측정                                          | 모델이 학습된 후 **성능 평가**를 위한 지표 제공                           |
| **학습 중 사용 여부** | 사용됨 (학습 과정을 통해 Loss가 줄어들도록 모델 최적화)                                         | 사용되지 않음 (학습 완료 후 최종 성능 평가에 사용)                        |
| **해석의 직관성**     | 수치적으로 모델의 최적화 정도를 나타내지만, 직관적인 성능 측정에는 불편                         | 더 직관적이며, 모델 성능에 대한 해석이 용이                                |
| **미분 가능 여부**    | 미분 가능해야 함. 모델 학습 시 Gradient Descent로 최적화에 도움                                 | 미분 가능 여부는 중요하지 않음. 모델 학습 완료 후 성능을 평가하는 목적    |
| **예시**              | MSE, Cross Entropy Loss, Huber Loss 등                                                         | Accuracy, Precision, Recall, F1 Score, ROC-AUC 등                        |

---

이와 같이 Loss와 Evaluation Metric은 모델의 성능을 다각도로 평가하고 최적화하는 데 있어 상호 보완적인 역할을 합니다. Loss는 모델의 최적화를 위한 도구이며, Evaluation Metric은 최적화된 모델이 실제로 얼마나 잘 작동하는지 확인하는 평가 도구라고 할 수 있습니다.