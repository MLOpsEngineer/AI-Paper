### 1. **L2 Loss (MSE Loss)** - Mean Squared Error Loss
- **정의**: $$L = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2$$
- **특징**:
  - 예측값 $\hat{y}$와 실제값 $y$의 차이를 제곱하여 평균한 값입니다.
  - 큰 오차일수록 더 큰 손실을 부여하기 때문에, **오차가 큰 데이터에 민감**하게 반응합니다.
- **장점**:
  - 손실 값이 오차 크기에 비례해 커지므로, 큰 오차에 대해 모델을 빠르게 수렴시키는 데 유리합니다.
  - 기울기(미분)가 선형적으로 커지므로 수렴 속도가 빠릅니다.
- **단점**:
  - **이상치(outlier)에 민감**합니다. 이상치가 손실에 큰 영향을 주기 때문에, 이상치가 많을 경우 모델이 해당 데이터에 과도하게 적응할 수 있습니다.

---

### 2. **L1 Loss (MAE Loss)** - Mean Absolute Error Loss
- **정의**: $$L = \frac{1}{N} \sum_{i=1}^N |y_i - \hat{y}_i|$$
- **특징**:
  - 예측값과 실제값의 차이를 절대값으로 계산한 후 평균을 구합니다.
  - **오차가 클수록 비례해서 커지지만**, MSE처럼 제곱을 하지 않으므로 큰 오차에 덜 민감합니다.
- **장점**:
  - 이상치(outlier)에 더 **robust**합니다. 절대값을 사용하므로 MSE보다 이상치에 덜 영향을 받습니다.
- **단점**:
  - 손실 함수가 0에서 미분 불가능합니다. 따라서 최적화가 어렵고, 기울기가 급격히 변하지 않아서 학습 수렴이 상대적으로 느릴 수 있습니다.

---

### 3. **Huber Loss**
- **정의**:  
  - Huber Loss는 **오차의 크기에 따라 MSE와 MAE를 절충**한 형태의 함수입니다.
  - 임계값 $\delta$를 기준으로 오차가 작을 때는 MSE처럼, 오차가 클 때는 MAE처럼 동작합니다.
  $$
  L = \begin{cases} 
      \frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| < \delta \\
      \delta \cdot (|y - \hat{y}| - \frac{\delta}{2}) & \text{if } |y - \hat{y}| \geq \delta 
   \end{cases}
$$
- **특징**:
  - 오차가 작은 경우는 MSE의 특성을 활용하여 빠르게 수렴하고, 큰 오차에 대해서는 MAE처럼 작동해 이상치에 덜 민감하도록 설계되었습니다.
- **장점**:
  - **MSE의 빠른 수렴** 속도와 **MAE의 이상치 강건성**을 모두 가지고 있어, 두 손실 함수의 장점을 결합한 형태입니다.
- **단점**:
  - **두 번째 미분이 불연속적**입니다. $\delta$의 경계점에서 기울기가 급격히 변하므로, 두 번째 미분이 불연속적이어서 일부 최적화 기법에서 덜 안정적으로 작동할 수 있습니다.

---

### 4. **Log-Cosh Loss**
- **정의**: $$L = \log(\cosh(y - \hat{y}))$$
- **특징**:
  - Log Cosh는 오차를 하이퍼볼릭 코사인 함수로 변환한 뒤 로그를 취해 계산합니다.
  - 작은 오차에서는 MSE와 비슷하게 작용하고, 큰 오차에서는 MAE처럼 작동하여 이상치에 강건합니다.
- **장점**:
  - Huber Loss와 유사하지만, 모든 구간에서 부드럽게 미분이 가능합니다. 따라서 Huber Loss보다 **매끄럽고 안정적인 기울기**를 제공하여 학습이 더 부드럽게 진행됩니다.
  - 다중 미분이 가능하므로, 최적화 과정에서 안정성을 더 확보할 수 있습니다.
- **단점**:
  - 계산이 더 복잡할 수 있으며, 구체적인 손실값 해석이 어렵습니다.

---

### 비교 요약
| Loss Function | 특징 | 장점 | 단점 |
|---------------|-------|------|------|
| **L2 Loss (MSE)** | 큰 오차에 민감 | 빠르게 수렴 가능 | 이상치에 민감 |
| **L1 Loss (MAE)** | 큰 오차에 덜 민감 | 이상치에 강건 | 0에서 미분 불가능, 느린 수렴 |
| **Huber Loss** | MSE와 MAE의 절충 | 빠른 수렴 + 이상치 강건 | 두 번째 미분 불연속 |
| **Log-Cosh Loss** | 전 구간 미분 가능 | 안정적 학습, 이상치 강건 | 계산 복잡성 |

---

### **1. 미분 가능성의 의미와 왜 중요한가?**

**미분 가능성**은 함수의 특정 지점에서 기울기(변화율)를 정확히 정의할 수 있다는 것을 의미합니다. 이는 기계 학습에서 **역전파(backpropagation)** 알고리즘을 사용할 때 매우 중요합니다. 역전파는 손실 함수의 기울기를 계산하여 신경망의 가중치를 업데이트하는 과정입니다. 따라서 손실 함수가 모든 지점에서 미분 가능해야 기울기를 계산하여 학습을 진행할 수 있습니다.

---

### **2. L1 Loss (MAE Loss)의 미분 가능성**

**L1 Loss**는 다음과 같이 정의됩니다:

$$
L = \frac{1}{N} \sum_{i=1}^N |y_i - \hat{y}_i|
$$

여기서 $|y_i - \hat{y}_i|$는 예측값과 실제값의 차이의 절대값입니다.

**왜 $x=0$에서 미분 불가능한가?**

- **절대값 함수 $|x|$**를 생각해봅시다.
  - $x > 0$일 때, $|x| = x$, 따라서 미분은 $f'(x) = 1$입니다.
  - $x < 0$일 때, $|x| = -x$, 따라서 미분은 $f'(x) = -1$입니다.
  - $x = 0$일 때, 좌우 미분값이 다릅니다:
    - 좌측 미분계수: $\lim_{h \to 0^-} \frac{|0 + h| - |0|}{h} = -1$
    - 우측 미분계수: $\lim_{h \to 0^+} \frac{|0 + h| - |0|}{h} = 1$
  - 좌우 미분계수가 다르기 때문에 $x = 0$에서 미분값이 정의되지 않습니다.

**역전파 과정에서의 영향**

- 신경망 학습에서는 기울기를 사용하여 가중치를 업데이트해야 합니다.
- $x = 0$에서 미분이 정의되지 않으면, 해당 지점에서의 기울기를 정확히 계산할 수 없습니다.
- 하지만 실제 구현에서는 이 문제를 해결하기 위해 **아次の 값(서브그레디언트)**을 사용하거나, $x = 0$에서의 기울기를 임의로 0으로 정의하는 등의 방법을 사용합니다.
- 그러나 이러한 접근은 기울기의 변화가 급격하게 변하는 문제를 완전히 해결하지 못해 **학습이 불안정하거나 수렴 속도가 느려질 수 있습니다**.

---

### **3. Huber Loss의 미분 가능성**

**Huber Loss**는 다음과 같이 정의됩니다:

$$
L = \begin{cases} 
      \frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
      \delta \cdot \left( |y - \hat{y}| - \frac{\delta}{2} \right) & \text{if } |y - \hat{y}| > \delta
   \end{cases}
$$

여기서 $\delta$는 임계값입니다.

**왜 한 번만 미분 가능하고 두 번째 미분은 불연속적인가?**

- **첫 번째 미분(기울기)**:
  - 오차가 $\delta$보다 작을 때와 클 때 각각의 미분을 계산하면, 두 구간에서의 첫 번째 미분값은 경계점 $|y - \hat{y}| = \delta$에서 **연속적**입니다.
  - 따라서 Huber Loss는 **한 번 미분 가능**합니다.

- **두 번째 미분(곡률)**:
  - 두 번째 미분을 계산하면, 경계점에서 값이 달라집니다.
    - 오차가 $\delta$보다 작을 때: 두 번째 미분은 상수값 $1$입니다.
    - 오차가 $\delta$보다 클 때: 두 번째 미분은 $0$입니다.
  - 경계점에서 두 번째 미분값이 불연속적으로 변하므로, **두 번째 미분은 불연속적**입니다.

**역전파 과정에서의 영향**

- 첫 번째 미분이 연속적이므로, 기울기를 계산하여 가중치를 업데이트하는 데는 문제가 없습니다.
- 하지만 두 번째 미분이 불연속적이라는 것은 **기울기의 변화율(곡률)이 급격하게 변한다는 의미**입니다.
- 일부 최적화 알고리즘(예: Newton's Method)은 두 번째 미분을 활용하므로, 이러한 알고리즘에서는 Huber Loss의 두 번째 미분 불연속성이 문제가 될 수 있습니다.
- 그러나 일반적인 **경사 하강법(Gradient Descent)** 기반의 최적화에서는 첫 번째 미분만 사용하므로 큰 문제가 되지 않습니다.

---

### **4. L2 Loss (MSE Loss)의 미분 가능성**

**L2 Loss**는 다음과 같이 정의됩니다:

$$
L = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2
$$

**왜 $x=0$에서도 미분 가능한가?**

- $f(x) = x^2$ 함수는 모든 실수 $x$에서 미분 가능합니다.
  - $f'(x) = 2x$
  - $x = 0$일 때도 $f'(0) = 0$으로 잘 정의됩니다.
- 따라서 L2 Loss는 **모든 지점에서 미분 가능**하며, 역전파 과정에서 기울기를 계산하는 데 문제가 없습니다.

**역전파 과정에서의 영향**

- 미분 가능성이 모든 지점에서 보장되므로, 기울기를 안정적으로 계산할 수 있습니다.
- 이는 학습이 **매끄럽고 수렴 속도가 빠르며**, 이상치에 민감하다는 특성과 함께 작용합니다.

---

### **5. 정리 및 추가 설명**

**왜 미분 불가능성이 문제가 되는가?**

- 미분 불가능한 지점에서는 기울기를 정확히 정의할 수 없으므로, 가중치 업데이트가 불안정해질 수 있습니다.
- 이는 학습 과정에서 **진동이나 수렴 속도 저하**로 이어질 수 있습니다.

**실제 구현에서는 어떻게 해결하는가?**

- **서브그레디언트(subgradient)**를 사용하여 미분 불가능한 지점에서도 기울기를 근사하여 계산합니다.
- 또는 **손실 함수를 약간 변형**하여 미분 가능하도록 만드는 방법도 있습니다.

**Log-Cosh Loss는 왜 여러 번 미분 가능한가?**

- Log-Cosh Loss는 다음과 같이 정의됩니다:

$$
L = \log \left( \cosh (y - \hat{y}) \right)
$$

- 이 함수는 $\cosh$ 함수의 성질 때문에 모든 차수의 미분이 연속적입니다.
  - $\cosh(x)$와 그 미분인 $\sinh(x)$는 모두 매끄러운 함수입니다.
- 따라서 역전파 과정에서 기울기와 곡률이 모두 연속적으로 변하여 **매우 안정적인 학습**이 가능합니다.

---

### **6. 결론**

- **L1 Loss**: 절대값 함수로 인해 $x=0$에서 미분 불가능하지만, 서브그레디언트를 사용하여 학습이 가능합니다. 그러나 기울기가 급격히 변해 학습이 불안정할 수 있습니다.
- **Huber Loss**: 첫 번째 미분은 연속적이지만, 두 번째 미분이 불연속적입니다. 일반적인 경사 하강법에서는 큰 문제가 없지만, 고차 미분을 사용하는 최적화에서는 영향이 있을 수 있습니다.
- **L2 Loss**: 모든 지점에서 미분 가능하며, 기울기가 선형적으로 증가하여 안정적인 학습이 가능합니다.
- **Log-Cosh Loss**: 모든 차수의 미분이 연속적이어서 매우 안정적인 학습이 가능합니다.

**추천 사항**

- 만약 이상치에 강건하면서도 안정적인 학습을 원한다면 **Log-Cosh Loss**를 고려해볼 수 있습니다.
- 일반적인 회귀 문제에서 빠른 수렴이 필요하고 이상치가 크지 않다면 **L2 Loss**를 사용할 수 있습니다.
- 이상치가 많고 기울기의 급격한 변화를 감수할 수 있다면 **L1 Loss**도 선택지입니다.


---

### **신경망 구조 및 가정**

- **신경망 구조**:
  - 입력 레이어
  - **히든 레이어 1**
  - **히든 레이어 2**
  - **히든 레이어 3**
  - 출력 레이어

- **가정**:
  - 단순화를 위해 각 레이어는 선형 변환과 활성화 함수를 거칩니다.
  - 활성화 함수로는 ReLU 함수를 사용합니다.
  - 단일 입력 데이터 $x$와 그에 대응하는 실제 출력 $y$를 사용합니다.
  - 학습률(learning rate) $\eta$는 0.01로 가정합니다.

---

### **1. 순전파(Forward Pass)**

1. **입력**: $x$
2. **히든 레이어 1 출력**:
   $$
   h_1 = \text{ReLU}(W_1 x + b_1)
$$
3. **히든 레이어 2 출력**:
   $$
   h_2 = \text{ReLU}(W_2 h_1 + b_2)
$$
4. **히든 레이어 3 출력**:
   $$
   h_3 = \text{ReLU}(W_3 h_2 + b_3)
$$
5. **예측값(출력 레이어)**:
   $$
   \hat{y} = W_4 h_3 + b_4
$$
   - 여기서는 출력 레이어에서 선형 활성화를 사용한다고 가정합니다.

---

### **2. 손실 함수 계산**

예측값 $\hat{y}$와 실제값 $y$의 차이를 이용하여 손실을 계산합니다.

#### **예시 값 설정**

- **실제값**: $y = 3.0$
- **예측값**: $\hat{y} = 2.5$
- **오차**: $e = y - \hat{y} = 0.5$

#### **각 손실 함수로 손실 계산**

1. **L2 Loss (MSE Loss)**:
   $$
   L_{\text{MSE}} = \frac{1}{2}(y - \hat{y})^2 = \frac{1}{2}(0.5)^2 = 0.125
$$

2. **L1 Loss (MAE Loss)**:
   $$
   L_{\text{MAE}} = |y - \hat{y}| = |0.5| = 0.5
$$

3. **Huber Loss** (임계값 $\delta = 1$로 가정):
   - $|e| = 0.5 \leq \delta$이므로,
   $$
   L_{\text{Huber}} = \frac{1}{2}(y - \hat{y})^2 = 0.125
$$

4. **Log-Cosh Loss**:
   $$
   L_{\text{Log-Cosh}} = \log\left( \cosh(y - \hat{y}) \right) = \log\left( \cosh(0.5) \right) \approx 0.1201
$$

---

### **3. 역전파(Backpropagation)**

손실 함수를 각 가중치에 대해 미분하여 기울기를 계산하고, 이를 이용해 가중치를 업데이트합니다.

#### **출력층에서의 기울기 계산**

각 손실 함수에 대해 예측값 $\hat{y}$에 대한 손실의 미분 $\frac{\partial L}{\partial \hat{y}}$를 계산합니다.

1. **L2 Loss**:
   $$
   \frac{\partial L_{\text{MSE}}}{\partial \hat{y}} = - (y - \hat{y}) = - e = -0.5
$$

2. **L1 Loss**:
   $$
   \frac{\partial L_{\text{MAE}}}{\partial \hat{y}} =
   \begin{cases}
   -1 & \text{if } e > 0 \\
   1 & \text{if } e < 0 \\
   \text{미분 불가능} & \text{if } e = 0
   \end{cases}
$$
   - 여기서는 $e = 0.5 > 0$이므로 $\frac{\partial L}{\partial \hat{y}} = -1$

3. **Huber Loss**:
   - $|e| \leq \delta$인 구간에서,
   $$
   \frac{\partial L_{\text{Huber}}}{\partial \hat{y}} = - (y - \hat{y}) = - e = -0.5
$$

4. **Log-Cosh Loss**:
   $$
   \frac{\partial L_{\text{Log-Cosh}}}{\partial \hat{y}} = - \tanh(y - \hat{y}) = - \tanh(0.5) \approx -0.4621
$$

#### **출력층 가중치 $W_4$ 업데이트**

가중치 업데이트 식:
$$
W_{\text{new}} = W_{\text{old}} - \eta \cdot \frac{\partial L}{\partial W}
$$

출력층에서:
$$
\frac{\partial L}{\partial W_4} = \frac{\partial L}{\partial \hat{y}} \cdot h_3^T
$$

- $h_3$는 히든 레이어 3의 출력입니다.

#### **히든 레이어에서의 기울기 계산**

역전파를 통해 이전 레이어로 기울기를 전파합니다.

1. **오차 신호 계산**:
   $$
   \delta_4 = \frac{\partial L}{\partial \hat{y}} \quad (\text{출력층})
$$
   $$
   \delta_3 = (\delta_4 \cdot W_4^T) \circ f'(z_3)
$$
   - $f'(z_3)$는 히든 레이어 3의 활성화 함수의 미분입니다.
   - ReLU 함수의 미분은 $z > 0$일 때 1, $z \leq 0$일 때 0입니다.

2. **가중치 기울기 계산**:
   $$
   \frac{\partial L}{\partial W_3} = \delta_3 \cdot h_2^T
$$
   - $h_2$는 히든 레이어 2의 출력입니다.

3. **이 과정을 히든 레이어 1까지 반복**:
   $$
   \delta_2 = (\delta_3 \cdot W_3^T) \circ f'(z_2)
$$
   $$
   \frac{\partial L}{\partial W_2} = \delta_2 \cdot h_1^T
$$
   $$
   \delta_1 = (\delta_2 \cdot W_2^T) \circ f'(z_1)
$$
   $$
   \frac{\partial L}{\partial W_1} = \delta_1 \cdot x^T
$$

#### **가중치 업데이트**

각 레이어의 가중치를 업데이트합니다.

- $W_l = W_l - \eta \cdot \frac{\partial L}{\partial W_l}$

---

### **4. 미분 가능성이 역전파에 미치는 영향**

#### **L1 Loss의 미분 불가능성**

- **문제점**: $e = y - \hat{y} = 0$일 때, $\frac{\partial L}{\partial \hat{y}}$가 정의되지 않습니다.
- **실제 영향**:
  - 학습 과정에서 예측값이 실제값과 정확히 같아지는 경우는 드뭅니다.
  - 하지만 만약 그런 경우가 발생하면, 기울기를 계산할 수 없으므로 가중치 업데이트에 문제가 생길 수 있습니다.
  - **해결 방법**:
    - 서브그래디언트(subgradient)를 사용하여 $e = 0$일 때 기울기를 0으로 정의합니다.
    - 또는 작은 값을 추가하여 미분 가능하도록 만듭니다.

#### **Huber Loss의 두 번째 미분 불연속성**

- **첫 번째 미분은 연속적**이므로 역전파 과정에서 기울기를 계산하는 데 문제가 없습니다.
- **두 번째 미분이 불연속적**인 것은 고차 미분을 사용하는 최적화 알고리즘에서만 문제가 됩니다.
- **역전파에는 영향이 적습니다**.

#### **L2 Loss와 Log-Cosh Loss의 미분 가능성**

- 두 손실 함수 모두 **모든 지점에서 미분 가능**하며, 기울기가 연속적입니다.
- **안정적인 학습**이 가능합니다.

---

### **5. 예제를 통한 전체 과정 요약**

#### **가정한 값**

- 입력값 $x$는 스칼라 또는 벡터입니다.
- 학습률 $\eta = 0.01$
- 초기 가중치와 편향은 임의의 값으로 설정됩니다.

#### **순서**

1. **순전파**를 통해 예측값 $\hat{y}$를 계산합니다.
2. **손실 함수**를 사용하여 손실 $L$을 계산합니다.
3. **손실의 미분** $\frac{\partial L}{\partial \hat{y}}$를 계산합니다.
4. **역전파**를 통해 각 레이어의 기울기 $\frac{\partial L}{\partial W_l}$를 계산합니다.
5. **가중치 업데이트**:
   $$
   W_l = W_l - \eta \cdot \frac{\partial L}{\partial W_l}
$$
6. **이 과정을 반복**하여 손실이 최소화되도록 학습합니다.

---

### **6. 결론 및 추가 설명**

- **미분 가능성은 역전파에서 매우 중요**합니다. 기울기를 계산하여 가중치를 업데이트하는 과정에서 미분 불가능한 지점이 있으면 학습이 불안정해질 수 있습니다.
- **L1 Loss**는 $e = 0$에서 미분 불가능하지만, 실제 학습에서는 서브그래디언트 등을 사용하여 문제를 해결합니다.
- **Huber Loss**는 첫 번째 미분이 연속적이므로 일반적인 역전파에서는 문제가 없습니다.
- **L2 Loss와 Log-Cosh Loss**는 모든 지점에서 미분 가능하여 안정적인 학습이 가능합니다.
- **히든 레이어의 수가 많아도** 역전파 과정은 동일하며, 기울기를 체인 룰(chain rule)을 통해 전파합니다.

---

### **추가적인 예시**

만약 **예측값과 실제값이 같아져서 오차 $e = 0$** 인 경우를 생각해봅시다.

- **L1 Loss**:
  - $e = 0$에서 미분 불가능하지만, 서브그래디언트를 사용하여 기울기를 0으로 정의할 수 있습니다.
  - 기울기가 0이 되므로 가중치 업데이트가 이루어지지 않습니다. 이는 이미 최적의 예측을 했다는 의미이므로 자연스러운 결과입니다.

- **L2 Loss**:
  - $e = 0$일 때도 미분 가능합니다.
  - $\frac{\partial L_{\text{MSE}}}{\partial \hat{y}} = - (y - \hat{y}) = 0$
  - 기울기가 0이므로 가중치 업데이트가 이루어지지 않습니다.

---

### **요약**

- 손실 함수의 선택은 모델의 성능과 학습 안정성에 큰 영향을 미칩니다.
- **미분 가능성**은 역전파에서 기울기를 계산하는 데 필수적입니다.
- **실제 구현에서는** 미분 불가능한 지점에 대한 처리를 통해 학습을 원활하게 진행합니다.
- **깊은 신경망에서도** 이 원리는 동일하게 적용되며, 각 레이어에서의 기울기를 계산하여 가중치를 업데이트합니다.

---
