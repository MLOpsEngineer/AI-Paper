**Attention의 원리**

- **텐서의 기본 성질**
  - **정보 보존**: 정보를 나타내는 텐서는 웨이트 매트릭스를 통과해도 기존의 정보를 유지합니다.
  - **웨이티드 섬 (Weighted Sum)**: 여러 정보 텐서를 가중합하면, 가중치가 큰 정보의 비중이 더 커집니다.
  - **내적 (Dot Product)**: 벡터 간의 내적은 유사도를 측정하며, 유사한 정보일수록 값이 큽니다.

- **Attention 메커니즘**
  - **쿼리와 키의 내적 계산**: 쿼리 텐서와 각 키 텐서의 내적을 통해 유사도를 구합니다.
  - **가중치 정규화**: 내적 결과를 익스포넨셜 함수와 소프트맥스 함수를 사용하여 양수로 변환하고, 합이 1이 되도록 정규화합니다.
  - **밸류의 가중합**: 정규화된 가중치를 밸류 텐서에 곱하여 가중합을 구합니다.
  - **결과**: 어텐션의 출력은 쿼리와 유사한 키의 밸류 정보를 더 많이 반영한 결과입니다.

- **쿼리, 키, 밸류의 역할**
  - **쿼리 (Query)**: 필요한 정보를 요청하는 역할을 합니다.
  - **키 (Key)**: 각 밸류의 인덱스나 식별자 역할을 합니다.
  - **밸류 (Value)**: 실제로 전달되는 정보입니다.

**Positional Encoding과 Self-Attention**

- **어텐션의 한계**
  - **순서 정보의 부재**: 기본 어텐션은 단어의 위치나 순서를 고려하지 못합니다.
  - **동음이의어 문제**: 문맥에 따라 의미가 달라지는 단어를 구별하지 못합니다.

- **Positional Encoding**
  - **위치 정보 주입**: 각 단어 벡터에 위치 정보를 나타내는 벡터를 더하여 순서 정보를 포함시킵니다.
  - **방법**: 첫 번째 단어에는 첫 번째 위치 벡터를, 두 번째 단어에는 두 번째 위치 벡터를 더하는 방식입니다.

- **Self-Attention**
  - **문맥 정보 반영**: 쿼리와 키를 동일한 입력으로 사용하여, 단어 간의 관계를 학습합니다.
  - **과정**: 여러 레이어를 거치면서 단어 벡터가 주변 단어들과의 맥락 정보를 포함하도록 업데이트됩니다.

**Transformer의 학습방식과 마스킹**

- **Transformer의 동작**
  - **인코더**: 입력 문장의 전체 정보를 사용하여 키와 밸류를 생성합니다.
  - **디코더**: 이전에 생성된 단어들을 기반으로 다음 단어를 예측합니다.

- **학습 시 문제점**
  - **미래 정보의 누설**: 디코더에서 전체 문장을 한 번에 사용하면, 아직 생성되지 않은 미래의 단어 정보를 참고하게 됩니다.
  - **해결 필요성**: 이는 모델이 실제 사용 시와 다른 조건에서 학습되는 문제를 발생시킵니다.

- **마스킹 (Masking)**
  - **미래 단어 가리기**: 디코더의 셀프 어텐션에서 미래 단어의 가중치를 0으로 만들어 영향을 없앱니다.
  - **구현 방법**: 미래 단어 위치의 내적 결과에 큰 음수 값을 더하여, 소프트맥스 후에 거의 0이 되도록 합니다.
  - **효과**: 모델이 현재까지의 단어만을 사용하여 다음 단어를 예측하게 됩니다.

**결론**

- **어텐션의 핵심**: 쿼리, 키, 밸류를 활용하여 중요한 정보를 집중적으로 반영합니다.
- **순서와 문맥의 중요성**: Positional Encoding과 Self-Attention을 통해 단어의 위치와 주변 관계를 학습합니다.
- **마스킹의 역할**: 학습과 추론 시의 조건을 일치시켜 모델의 일반화 성능을 향상시킵니다.
- **트랜스포머의 장점**: 이러한 메커니즘을 통해 복잡한 자연어 처리 문제에서 우수한 성능을 보입니다.