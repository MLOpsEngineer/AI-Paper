### 1. Loss Function (손실 함수)
손실 함수는 모델이 예측한 값과 실제 값 간의 차이를 수치화하여 모델의 성능을 평가하는 지표입니다. 예를 들어, 분류 문제에서는 **Cross Entropy Loss**를, 회귀 문제에서는 **MSE (Mean Squared Error)** 또는 **MAE (Mean Absolute Error)**를 자주 사용합니다. 손실 함수의 값이 작을수록 모델의 예측이 정답에 가까워지므로, 경사하강법을 사용하여 이 값을 최소화하는 방향으로 학습을 진행합니다.

### 2. 경사하강법 (Gradient Descent)
경사하강법은 손실 함수의 값이 줄어드는 방향으로 **모델의 파라미터(가중치)**를 갱신하는 방법입니다. 손실 함수의 **경사(기울기)**를 계산해 경사가 내려가는 방향으로 이동함으로써 손실을 줄입니다. 

#### 경사의 의미
- **경사(Gradient)**는 손실 함수의 변화율을 의미합니다. 즉, 어떤 변수의 변화에 따라 손실이 얼마나 변하는지를 나타냅니다.
- 경사가 **양수**일 때는 해당 방향으로 이동하면 손실이 증가하고, 경사가 **음수**일 때는 해당 방향으로 이동하면 손실이 감소합니다.
  
수식으로 표현하면 다음과 같습니다.
$$
\text{기울기 } g = \frac{\Delta y}{\Delta x} = \frac{y_2 - y_1}{x_2 - x_1}
$$
경사하강법에서는 이 기울기를 기반으로, 손실이 줄어드는 방향으로 **가중치(weight)**를 조정합니다.

---

### 3. Gradient Descent의 Update Rule
경사하강법은 다음의 업데이트 규칙에 따라 가중치를 조정합니다.
$$
w_{i+1} = w_i - \lambda \frac{dL}{dw}
$$
여기서:
- $w$: 가중치
- $L$: 손실 함수
- $\lambda$: **Learning Rate** (학습률), 가중치 변화의 크기를 조절하는 값

#### Learning Rate (학습률)
- **학습률**은 가중치가 얼마나 빠르게 갱신되는지를 결정합니다. 학습률이 **너무 크면** 최적점에 도달하지 못하고 발산할 수 있으며, **너무 작으면** 학습이 느려지고 최적점에 도달하기까지 시간이 오래 걸릴 수 있습니다.
- 학습률의 크기에 따른 변화:
  - **너무 큼**: 손실이 발산할 가능성이 큼.
  - **너무 작음**: 손실이 서서히 감소하여 학습 속도가 느림.
  - **적절한 크기**: 손실이 빠르게 감소하면서 안정적으로 최적점에 수렴함.
- 일반적으로 0.1부터 $1e-5$까지 다양한 값들이 사용되며, 모델에 따라 적절한 학습률을 찾기 위해 **초기 실험**이나 **하이퍼파라미터 최적화** 기법을 사용할 수 있습니다.

---

### 4. Gradient Descent의 종류
경사하강법에는 여러 변형이 있으며, 데이터의 사용 방식에 따라 다음과 같은 방법으로 나뉩니다.

1. **Stochastic Gradient Descent (SGD)**
   - 한 번에 하나의 데이터 샘플에 대해 가중치를 업데이트합니다.
   - 계산 속도가 빠르지만, 손실 함수가 매번 데이터 샘플에 따라 많이 요동치기 때문에 불안정할 수 있습니다.

2. **Full-batch Gradient Descent**
   - 전체 데이터셋을 한 번에 사용하여 가중치를 업데이트합니다.
   - 계산 비용이 높지만, 매번 안정적으로 손실 함수가 감소하는 경향이 있습니다.

3. **Mini-batch Gradient Descent**
   - 데이터셋을 작은 배치(batch) 단위로 나누어 업데이트를 수행합니다.
   - Full-batch와 SGD의 장점을 조합한 방식으로, 일반적으로 가장 많이 사용됩니다.
   - 배치 크기 $B$가 크면 손실 함수의 그라디언트를 더 정확하게 근사하고, 작을수록 노이즈가 많아져 **Saddle Point**와 같은 지점에서 탈출하기 쉽습니다.

---

### 5. Saddle Point (안장점)
- **안장점**은 특정 방향에서는 극대값이지만, 다른 방향에서는 극소값이 되는 지점입니다. 손실 함수의 경사가 0이 되어 가중치 업데이트가 멈출 수 있습니다.
- Mini-batch Gradient Descent는 데이터에 **노이즈**가 포함되기 때문에, 안장점에서도 학습이 진행될 수 있으며, 안정적으로 극소값으로 수렴하도록 도와줍니다.

---

### 6. Mini-batch 크기의 효과
- **Mini-batch 크기 $B$**가 클수록 전체 데이터에 대한 Loss Gradient를 잘 근사할 수 있으며, 학습이 좀 더 안정적입니다.
- 반면, $B$가 작을수록 손실 함수의 그라디언트가 "노이즈가 있는 상태"가 되어 Stochasticity가 증가합니다.
- 작고 다양한 배치를 사용하면, 모델이 다양한 국소 최적점을 탐색하게 되어 더 나은 최적점에 도달할 가능성이 높아집니다.

---

### 요약
경사하강법은 손실 함수의 값이 줄어드는 방향으로 가중치를 갱신하며, 이때 학습률(Learning Rate)을 조절하여 학습의 속도와 안정성을 결정합니다. 또한, 데이터 처리 방식에 따라 SGD, Full-batch, Mini-batch 방식으로 나뉘며, Mini-batch 방식은 안장점 문제를 피할 수 있는 장점이 있습니다.

경사하강법은 **모델의 예측 성능을 최적화하는** 핵심 방법으로, **적절한 학습률**과 **배치 크기**를 설정하는 것이 중요합니다.